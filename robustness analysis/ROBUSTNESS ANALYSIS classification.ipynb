{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from itertools import combinations\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "\n",
    "# PLOTTING\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "# CLASSIFICATION\n",
    "from glmnet import LogitNet\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from schemata.classification import get_pca_transformed_data, Classifier\n",
    "\n",
    "# MULTI PROCESSING\n",
    "import multiprocessing\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphometric statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = LogitNet(random_state=17, **{\"alpha\":0.5, \"standardize\":0})\n",
    "types = pd.DataFrame.from_csv('./data/types.csv').sort_values('c_num')\n",
    "def classify_morphometry(j,comb=('BC','DBC'), part='axon'):\n",
    "    path = './data/'+part+'/'\n",
    "\n",
    "    #get data \n",
    "    morph_idx = ['c_num', 'branch_points', 'tips', 'height', 'width', 'depth', \n",
    "                 'stems', 'avg_thickness', 'max_thickness', 'total_length', \n",
    "                 'total_surface', 'total_volume', 'max_path_dist_to_soma', \n",
    "                 'max_branch_order', 'max_segment_path_length', 'median_intermediate_segment_pl', \n",
    "                 'median_terminal_segment_pl', 'median_path_angle', \n",
    "                 'max_path_angle', 'median_tortuosity', 'max_tortuosity', \n",
    "                 'min_branch_angle', 'mean_branch_angle', 'max_branch_angle', 'max_degree', 'tree_asymmetry']\n",
    "    \n",
    "    data = pd.read_csv(path+'/morphometry_truncated_%0.2f.csv'%(j*0.1))\n",
    "    data.sort_values('c_num')\n",
    "    data = data[morph_idx]\n",
    "    idx = (types['type'] == comb[0]) | (types['type'] == comb[1])\n",
    "    data = data[idx.values]\n",
    "   \n",
    "    y = types[idx]['type'].values\n",
    "    y_shuffled = copy.copy(y)\n",
    "    np.random.shuffle(y_shuffled)\n",
    "\n",
    "    X = data.set_index(['c_num']).values\n",
    "    pca = PCA(copy=True, whiten=False)\n",
    "    kf = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=17)\n",
    "    runs = []\n",
    "    k = 1\n",
    "    fm_lengths = np.array([0] + [1]*X.shape[1])\n",
    "    for train_ix, test_ix in kf.split(X, y):\n",
    "\n",
    "        X_train = X[train_ix]\n",
    "        y_train = y[train_ix]\n",
    "        X_test = X[test_ix]\n",
    "        y_test = y[test_ix]\n",
    "\n",
    "        X_train, X_test = get_pca_transformed_data(X_train, X_test, pca, fm_lengths)\n",
    "        r_ = dict(truncated=j*10)\n",
    "        r_['run_id'] = k\n",
    "\n",
    "\n",
    "        m.fit(X_train, y_train)\n",
    "        \n",
    "        r_['accuracy_train'] = m.score(X_train, y_train)\n",
    "        r_['accuracy_test'] = m.score(X_test, y_test)\n",
    "        r_['log_loss_train'] = log_loss(y_train, m.predict_proba(X_train), labels=np.unique(y_train))\n",
    "        r_['log_loss_test'] = log_loss(y_test, m.predict_proba(X_test), labels=np.unique(y_test))\n",
    "        r_['shuffled'] = False\n",
    "        \n",
    "        runs.append(r_)\n",
    "        k += 1\n",
    "        \n",
    "    #redo on shuffled labels\n",
    "    k=1\n",
    "    for train_ix, test_ix in kf.split(X, y_shuffled):\n",
    "\n",
    "        X_train = X[train_ix]\n",
    "        y_train = y_shuffled[train_ix]\n",
    "        X_test = X[test_ix]\n",
    "        y_test = y_shuffled[test_ix]\n",
    "\n",
    "        X_train, X_test = get_pca_transformed_data(X_train, X_test, pca, fm_lengths)\n",
    "        r_ = dict(truncated=j*10)\n",
    "        r_['run_id'] = k\n",
    "\n",
    "\n",
    "        m.fit(X_train, y_train)\n",
    "        \n",
    "        r_['accuracy_train'] = m.score(X_train, y_train)\n",
    "        r_['accuracy_test'] = m.score(X_test, y_test)\n",
    "        r_['log_loss_train'] = log_loss(y_train, m.predict_proba(X_train), labels=np.unique(y_train))\n",
    "        r_['log_loss_test'] = log_loss(y_test, m.predict_proba(X_test), labels=np.unique(y_test))\n",
    "        r_['shuffled'] = True\n",
    "        \n",
    "        runs.append(r_)\n",
    "        k += 1\n",
    "\n",
    "    classification = pd.DataFrame(runs)\n",
    "    save_path = './results/'+part+'/'+comb[0]+' vs '+comb[1]+'/'\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    classification.to_csv(save_path+'classification_morphometry_truncated_%0.2f.csv'%(j*0.1))\n",
    "    \n",
    "    # train on entire data set and save the model and data\n",
    "    X_, _ = get_pca_transformed_data(X,X,pca,fm_lengths)\n",
    "    m.fit(X_,y)\n",
    "    \n",
    "    #open(save_path+'classification_morphometry_truncated_%0.2f_model'%(j*0.1),'wb').write(pickle.dumps(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPES = ['BC', 'BPC', 'BTC', 'ChC', 'DBC', 'MC', 'NGC']\n",
    "part='full'\n",
    "truncations = np.array(range(0,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in combinations(TYPES,2):\n",
    "    with multiprocessing.Pool(10) as pool:\n",
    "        results = pool.map(partial(classify_morphometry,comb=c,part='full'),truncations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm, gaussian_kde\n",
    "def get_persistence_image(data, dim=2, t=1, bins=100, xmax=None, xmin=None,  ymax=None, ymin=None):\n",
    "\n",
    "    if not data.empty:\n",
    "        if dim == 1:\n",
    "            y = np.zeros((bins,))\n",
    "            if xmax is None:\n",
    "                xmax = np.max(data['birth'])\n",
    "            if xmin is None:\n",
    "                xmin = np.min(data['birth'])\n",
    "            x = np.linspace(xmin, xmax, bins)\n",
    "\n",
    "            for k, p in data.iterrows():\n",
    "                m = np.abs(p['birth'] - p['death'])\n",
    "                y += m * norm.pdf(x, loc=p['birth'], scale=t)\n",
    "            return y, x\n",
    "\n",
    "        elif dim == 2:\n",
    "\n",
    "            try:\n",
    "                kernel = gaussian_kde(data[['birth', 'death']].values.T)\n",
    "            except LinAlgError as e:\n",
    "                print('Warning! Captured error %s'%e)\n",
    "                print('Add one additional data point.')\n",
    "\n",
    "                birth = data['birth'].subtract(1)\n",
    "                death = data['death'].add(1)\n",
    "                data = pd.concat((birth, death), axis=1)\n",
    "                data = data.append(pd.DataFrame(dict(birth=np.max(data['birth']+1), death=0), index=[len(data)]))\n",
    "\n",
    "                kernel = gaussian_kde(data[['birth', 'death']].values.T)\n",
    "\n",
    "            if xmax is None:\n",
    "                xmax = np.max(data['birth'].values)\n",
    "            if xmin is None:\n",
    "                xmin = np.min(data['birth'].values)\n",
    "            if ymax is None:\n",
    "                ymax = np.max(data['death'].values)\n",
    "            if ymin is None:\n",
    "                ymin = np.min(data['death'].values)\n",
    "\n",
    "            X, Y = np.mgrid[xmin:xmax:np.complex(bins), ymin:ymax:np.complex(bins)]\n",
    "            positions = np.vstack([X.ravel(), Y.ravel()])\n",
    "\n",
    "            Z = np.reshape(kernel(positions).T, X.shape)\n",
    "            return Z, [np.unique(X), np.unique(Y)]\n",
    "\n",
    "\n",
    "m = LogitNet(random_state=17, **{\"alpha\":0.5, \"standardize\":0})\n",
    "types = pd.DataFrame.from_csv('./data/types.csv').sort_values('c_num')\n",
    "\n",
    "def classify_persistence(j, comb=('BC','DBC'), part='axon'):\n",
    "    \n",
    "    path = './data/'+part+'/'\n",
    "    \n",
    "    #get data \n",
    "    data_all = pd.read_csv(path+'persistence_truncated_%0.2f.csv'%(j*0.1))\n",
    "    \n",
    "    idx = (types['type'] == comb[0]) | (types['type'] == comb[1])\n",
    "    data = pd.DataFrame()\n",
    "    for c in types[idx]['c_num'].values:\n",
    "        data = data.append(data_all[data_all['c_num'] == c])\n",
    "   \n",
    "    y = types[idx]['type'].values\n",
    "    # shuffle labels for shuffled baseline\n",
    "    y_shuffled = copy.copy(y)\n",
    "    np.random.shuffle(y_shuffled)\n",
    "    \n",
    "    # get normalization boundaries and parameter for persistance image generation\n",
    "    c_nums = np.unique(types[idx]['c_num'])\n",
    "    max_birth = np.ceil(np.max(data_all['birth']))\n",
    "    max_death = np.ceil(np.max(data_all['death']))\n",
    "    \n",
    "    min_birth = np.floor(np.min(data_all['birth']))\n",
    "    min_death = np.floor(np.min(data_all['death']))\n",
    "    dim=2\n",
    "    bins=100\n",
    "    X = np.zeros((len(c_nums),bins**dim))\n",
    "\n",
    "    # generate persistence images\n",
    "    for k,num in enumerate(c_nums):\n",
    "        try:\n",
    "            X[k,:] = get_persistence_image(data[data['c_num'] ==num], xmin=min_birth, xmax=max_birth, \n",
    "                                           ymin=min_death, ymax=max_death, dim=dim, bins=bins)[0].reshape(-1)\n",
    "        except np.linalg.LinAlgError as e:\n",
    "            return 'no image calculation possible anymore'\n",
    "    \n",
    "    X[X == np.nan] = 0\n",
    "    X[X == np.inf] = 0\n",
    "    X[X == -np.inf] = 0\n",
    "    \n",
    "    # Classification \n",
    "    pca = PCA(copy=True, whiten=False)\n",
    "    kf = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=17)\n",
    "\n",
    "    runs = []\n",
    "    k = 1\n",
    "    fm_lengths=np.array([0,X[0].shape[0]])\n",
    "    # regular Classification\n",
    "    for train_ix, test_ix in kf.split(X, y):\n",
    "\n",
    "        X_train = X[train_ix]\n",
    "        y_train = y[train_ix]\n",
    "        X_test = X[test_ix]\n",
    "        y_test = y[test_ix]\n",
    "        \n",
    "        try:\n",
    "            X_train, X_test = get_pca_transformed_data(X_train, X_test, pca, fm_lengths)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            return 'no image calculation possible anymore'\n",
    "        r_ = dict(truncated=j*10)\n",
    "        r_['run_id'] = k\n",
    "\n",
    "        m.fit(X_train, y_train)\n",
    "\n",
    "        r_['accuracy_train'] = m.score(X_train, y_train)\n",
    "        r_['accuracy_test'] = m.score(X_test, y_test)\n",
    "        r_['log_loss_train'] = log_loss(y_train, m.predict_proba(X_train), labels=np.unique(y_train))\n",
    "        r_['log_loss_test'] = log_loss(y_test, m.predict_proba(X_test), labels=np.unique(y_test))\n",
    "        r_['shuffled'] = False\n",
    "        runs.append(r_)\n",
    "        k += 1\n",
    "    k=1 \n",
    "    # Classification on shuffled data\n",
    "    for train_ix, test_ix in kf.split(X, y_shuffled):\n",
    "\n",
    "        X_train = X[train_ix]\n",
    "        y_train = y_shuffled[train_ix]\n",
    "        X_test = X[test_ix]\n",
    "        y_test = y_shuffled[test_ix]\n",
    "        \n",
    "        try:\n",
    "            X_train, X_test = get_pca_transformed_data(X_train, X_test, pca, fm_lengths)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            return 'no image calculation possible anymore'\n",
    "        r_ = dict(truncated=j*10)\n",
    "        r_['run_id'] = k\n",
    "\n",
    "        m.fit(X_train, y_train)\n",
    "\n",
    "        r_['accuracy_train'] = m.score(X_train, y_train)\n",
    "        r_['accuracy_test'] = m.score(X_test, y_test)\n",
    "        r_['log_loss_train'] = log_loss(y_train, m.predict_proba(X_train), labels=np.unique(y_train))\n",
    "        r_['log_loss_test'] = log_loss(y_test, m.predict_proba(X_test), labels=np.unique(y_test))\n",
    "        r_['shuffled'] = True\n",
    "        runs.append(r_)\n",
    "        k += 1\n",
    "\n",
    "    classification = pd.DataFrame(runs)\n",
    "    save_path = './results/'+part+'/'+comb[0]+' vs '+comb[1]+'/'\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "    classification.to_csv(save_path+'classification_persistence_truncated_%0.2f.csv'%(j*0.1))\n",
    "    \n",
    "    # train on entire data set and save the model and data\n",
    "    try:\n",
    "        X_, _ = get_pca_transformed_data(X,X,pca,fm_lengths)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return 'no image calculation possible for all data'\n",
    "    m.fit(X_,y)\n",
    "    \n",
    "    #open(save_path + 'classification_persistence_truncated_%0.2f_model'%(j*0.1),'wb').write(pickle.dumps(m))\n",
    "    #open(save_path + 'classification_persistence_truncated_%0.2f_pca'%(j*0.1),'wb').write(pickle.dumps(pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in list(combinations(TYPES,2)):\n",
    "    with multiprocessing.Pool(5) as pool:\n",
    "        try:\n",
    "            results = pool.map(partial(classify_persistence,comb=c,part='full'),truncations)\n",
    "        except ValueError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Density maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _project_data(dim, proj_axes, data):\n",
    "\n",
    "    p_a = proj_axes\n",
    "    if dim == 2:\n",
    "        if len(p_a) < 2:\n",
    "            print('Invalid parameter setting: The passed projection axes {0} do not \\\n",
    "                  fit with the dimension of the projection {1}'.format(p_a, dim))\n",
    "        else:\n",
    "            indices = '012'\n",
    "            for ix in range(len(p_a)):\n",
    "                indices = indices.replace(p_a[ix], '')\n",
    "            deleted_axis = int(indices)\n",
    "            ax = [0, 1, 2]\n",
    "            ax.remove(deleted_axis)\n",
    "            result = data[:, ax]\n",
    "\n",
    "    elif dim == 1:\n",
    "        if len(p_a) > 1:\n",
    "            print('Invalid parameter setting: The passed projection axes {0} do not \\\n",
    "                  fit with the dimension of the projection {1}'.format(p_a, dim))\n",
    "            \n",
    "        else:\n",
    "            ax = int(p_a)\n",
    "            result = data[:, ax]\n",
    "    else:\n",
    "        result = data\n",
    "\n",
    "    return result\n",
    "\n",
    "from utils.utils import smooth_gaussian\n",
    "def get_density_map(pc,proj_axes, r):\n",
    "    \n",
    "\n",
    "    dim = len(proj_axes)\n",
    "    pc = (pc - r['min'][0]) / (r['max'][0] - r['min'][0])\n",
    "\n",
    "    data = _project_data(dim, proj_axes, pc)\n",
    "\n",
    "    range_ = [[-.1, 1.1]] * dim\n",
    "    H, edges = np.histogramdd(data, bins=(100,) * dim,\n",
    "                              range=range_, normed=True)\n",
    "\n",
    "    H = smooth_gaussian(H, dim=dim, sigma=2)\n",
    "    return H\n",
    "\n",
    "\n",
    "m = LogitNet(random_state=17, **{\"alpha\":0.5, \"standardize\":0})\n",
    "\n",
    "def classify_density_map(j, comb=('BC','DBC'), part='axon'):\n",
    "    \n",
    "    path = './data/'+part+'/'\n",
    "    \n",
    "    #get data \n",
    "    data_all = pd.read_csv(path+'point_cloud_truncated_%0.2f.csv'%(j*0.1))\n",
    "    \n",
    "    idx = (types['type'] == comb[0]) | (types['type'] == comb[1])\n",
    "    data = pd.DataFrame()\n",
    "    for c in types[idx]['c_num'].values:\n",
    "        data = data.append(data_all[data_all['c_num'] == c])\n",
    "   \n",
    "    y = types[idx]['type'].values\n",
    "    y_shuffled = copy.copy(y)\n",
    "    np.random.shuffle(y_shuffled)\n",
    "    \n",
    "    c_nums = np.unique(types[idx]['c_num'])\n",
    "    r = dict(min=np.min(data_all[['x','y','z']]).values, max=np.max(data_all[['x','y','z']]).values)\n",
    "    X = np.zeros((len(c_nums),10000))\n",
    "\n",
    "    for k,num in enumerate(c_nums):\n",
    "        \n",
    "        D = data[data['c_num'] ==num][['x','y','z']].values\n",
    "        X[k,:] = get_density_map(D, '02', r).reshape(-1)\n",
    "\n",
    "    X[X == np.nan] = 0\n",
    "    X[X == np.inf] = 0\n",
    "    X[X == -np.inf] = 0\n",
    "    \n",
    "    \n",
    "    #### CLASSIFICATION ######\n",
    "    pca = PCA(copy=True, whiten=False)    \n",
    "    kf = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=17)\n",
    "\n",
    "    runs = []\n",
    "    k = 1\n",
    "    fm_lengths=np.array([0,X[0].shape[0]])\n",
    "    for train_ix, test_ix in kf.split(X, y):\n",
    "\n",
    "        X_train = X[train_ix]\n",
    "        y_train = y[train_ix]\n",
    "        X_test = X[test_ix]\n",
    "        y_test = y[test_ix]\n",
    "        \n",
    "        try:\n",
    "            X_train, X_test = get_pca_transformed_data(X_train, X_test, pca, fm_lengths)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            return 'no image calculation possible anymore'\n",
    "        r_ = dict(truncated=j*10)\n",
    "        r_['run_id'] = k\n",
    "\n",
    "        m.fit(X_train, y_train)\n",
    "\n",
    "        r_['accuracy_train'] = m.score(X_train, y_train)\n",
    "        r_['accuracy_test'] = m.score(X_test, y_test)\n",
    "        r_['log_loss_train'] = log_loss(y_train, m.predict_proba(X_train), labels=np.unique(y_train))\n",
    "        r_['log_loss_test'] = log_loss(y_test, m.predict_proba(X_test), labels=np.unique(y_test))\n",
    "        r_['shuffled'] = False\n",
    "        runs.append(r_)\n",
    "        k += 1\n",
    "    k=1 \n",
    "    for train_ix, test_ix in kf.split(X, y_shuffled):\n",
    "\n",
    "        X_train = X[train_ix]\n",
    "        y_train = y_shuffled[train_ix]\n",
    "        X_test = X[test_ix]\n",
    "        y_test = y_shuffled[test_ix]\n",
    "        \n",
    "        try:\n",
    "            X_train, X_test = get_pca_transformed_data(X_train, X_test, pca, fm_lengths)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            return 'no image calculation possible anymore'\n",
    "        r_ = dict(truncated=j*10)\n",
    "        r_['run_id'] = k\n",
    "\n",
    "        m.fit(X_train, y_train)\n",
    "\n",
    "        r_['accuracy_train'] = m.score(X_train, y_train)\n",
    "        r_['accuracy_test'] = m.score(X_test, y_test)\n",
    "        r_['log_loss_train'] = log_loss(y_train, m.predict_proba(X_train), labels=np.unique(y_train))\n",
    "        r_['log_loss_test'] = log_loss(y_test, m.predict_proba(X_test), labels=np.unique(y_test))\n",
    "        r_['shuffled'] = True\n",
    "        runs.append(r_)\n",
    "        k += 1\n",
    "\n",
    "    classification = pd.DataFrame(runs)\n",
    "    save_path = './results/'+part+'/'+comb[0]+' vs '+comb[1]+'/'\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "    classification.to_csv(save_path+'classification_density_map_truncated_%0.2f.csv'%(j*0.1))\n",
    "    \n",
    "    # train on entire data set and save the model and data\n",
    "    try:\n",
    "        X_, _ = get_pca_transformed_data(X,X,pca,fm_lengths)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return 'no image calculation possible for all data'\n",
    "    m.fit(X_,y)\n",
    "    \n",
    "    #open(save_path + 'classification_density_map_truncated_%0.2f_model'%(j*0.1),'wb').write(pickle.dumps(m))\n",
    "    #open(save_path + 'classification_density_map_truncated_%0.2f_pca'%(j*0.1),'wb').write(pickle.dumps(pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in list(combinations(TYPES,2)):\n",
    "    with multiprocessing.Pool(5) as pool:\n",
    "        try:\n",
    "            results = pool.map(partial(classify_density_map,comb=c,part='full'),truncations)\n",
    "        except ValueError:\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
