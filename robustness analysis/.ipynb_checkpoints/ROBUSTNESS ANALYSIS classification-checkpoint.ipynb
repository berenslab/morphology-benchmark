{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from itertools import combinations\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "\n",
    "# PLOTTING\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "# CLASSIFICATION\n",
    "from glmnet import LogitNet\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "# MULTI PROCESSING\n",
    "import multiprocessing\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphometric statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca_transformed_data(X_trn, X_tst, pca, fm_lengths, var=.9, scaling=True):\n",
    "    indices = np.cumsum(fm_lengths)\n",
    "\n",
    "    X_train_ = np.zeros((X_trn.shape))\n",
    "    X_test_ = np.zeros((X_tst.shape))\n",
    "    std_dev = np.ones(X_trn.shape[1])\n",
    "\n",
    "    # how many single value stats are included?\n",
    "    u, c = np.unique(fm_lengths, return_counts=True)\n",
    "\n",
    "    o_start = 0\n",
    "    for k in range(1, fm_lengths.shape[0]):\n",
    "        start = indices[k - 1]\n",
    "        stop = indices[k]\n",
    "\n",
    "        if stop - start > 1:  # it's not a morphometric\n",
    "            pca_X = pca.fit_transform(X_trn[:, start:stop])\n",
    "            idx = np.argmax(np.cumsum(pca.explained_variance_ratio_) > var) + 1\n",
    "\n",
    "            X_train_[:, o_start:o_start + idx] = pca_X[:, :idx]\n",
    "            X_test_[:, o_start:o_start + idx] = pca.transform(X_tst[:, start:stop])[:, :idx]\n",
    "            if scaling:\n",
    "                # scale by std of first component for when the features are combined\n",
    "                X_train_[:, o_start:o_start + idx] /= np.std(pca_X[:, 0])\n",
    "                X_test_[:, o_start:o_start + idx] /= np.std(pca_X[:, 0])\n",
    "\n",
    "        else:\n",
    "            idx = 1\n",
    "            if c[u == 1] > 1:\n",
    "                std_dev[o_start:o_start + idx] = get_std_dev(X_trn[:, start:stop])\n",
    "            X_train_[:, o_start:o_start + idx] = X_trn[:, start:stop]\n",
    "            X_test_[:, o_start:o_start + idx] = X_tst[:, start:stop]\n",
    "\n",
    "        o_start += idx\n",
    "    o_stop = copy.copy(o_start)\n",
    "\n",
    "    # perform z-scoring. If no morphometrics were involved the std_dev only contains ones\n",
    "    X_train = copy.copy(X_train_[:, :o_stop]) / std_dev[:o_stop]\n",
    "    X_test = copy.copy(X_test_[:, :o_stop]) / std_dev[:o_stop]\n",
    "\n",
    "    if len(X_train.shape) > 2:\n",
    "        X_train.squeeze(), X_test.squeeze()\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = LogitNet(random_state=17, **{\"alpha\":0.5, \"standardize\":0})\n",
    "types = pd.DataFrame.from_csv('./data/types.csv').sort_values('c_num')\n",
    "def classify_morphometry(j,comb=('BC','DBC'), part='axon'):\n",
    "    path = './data/'+part+'/'\n",
    "\n",
    "    #get data \n",
    "    morph_idx = ['c_num', 'branch_points', 'tips', 'height', 'width', 'depth', \n",
    "                 'stems', 'avg_thickness', 'max_thickness', 'total_length', \n",
    "                 'total_surface', 'total_volume', 'max_path_dist_to_soma', \n",
    "                 'max_branch_order', 'max_segment_path_length', 'median_intermediate_segment_pl', \n",
    "                 'median_terminal_segment_pl', 'median_path_angle', \n",
    "                 'max_path_angle', 'median_tortuosity', 'max_tortuosity', \n",
    "                 'min_branch_angle', 'mean_branch_angle', 'max_branch_angle', 'max_degree', 'tree_asymmetry']\n",
    "    \n",
    "    data = pd.DataFrame.from_csv(path+'/morphometry_truncated_%0.2f.csv'%(j*0.1))\n",
    "    data.sort_values('c_num')\n",
    "    data = data[morph_idx]\n",
    "    idx = (types['type'] == comb[0]) | (types['type'] == comb[1])\n",
    "    data = data[idx.values]\n",
    "   \n",
    "    y = types[idx]['type'].values\n",
    "    y_shuffled = copy.copy(y)\n",
    "    np.random.shuffle(y_shuffled)\n",
    "\n",
    "    X = data.set_index(['c_num']).values\n",
    "    pca = PCA(copy=True, whiten=False)\n",
    "    kf = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=17)\n",
    "    runs = []\n",
    "    k = 1\n",
    "    fm_lengths = np.array([0] + [1]*X.shape[1])\n",
    "    for train_ix, test_ix in kf.split(X, y):\n",
    "\n",
    "        X_train = X[train_ix]\n",
    "        y_train = y[train_ix]\n",
    "        X_test = X[test_ix]\n",
    "        y_test = y[test_ix]\n",
    "\n",
    "        X_train, X_test = get_pca_transformed_data(X_train, X_test, pca, fm_lengths)\n",
    "        r_ = dict(truncated=j*10)\n",
    "        r_['run_id'] = k\n",
    "\n",
    "\n",
    "        m.fit(X_train, y_train)\n",
    "        \n",
    "        r_['accuracy_train'] = m.score(X_train, y_train)\n",
    "        r_['accuracy_test'] = m.score(X_test, y_test)\n",
    "        r_['log_loss_train'] = log_loss(y_train, m.predict_proba(X_train), labels=np.unique(y_train))\n",
    "        r_['log_loss_test'] = log_loss(y_test, m.predict_proba(X_test), labels=np.unique(y_test))\n",
    "        r_['shuffled'] = False\n",
    "        \n",
    "        runs.append(r_)\n",
    "        k += 1\n",
    "        \n",
    "    #redo on shuffled labels\n",
    "    k=1\n",
    "    for train_ix, test_ix in kf.split(X, y_shuffled):\n",
    "\n",
    "        X_train = X[train_ix]\n",
    "        y_train = y_shuffled[train_ix]\n",
    "        X_test = X[test_ix]\n",
    "        y_test = y_shuffled[test_ix]\n",
    "\n",
    "        X_train, X_test = get_pca_transformed_data(X_train, X_test, pca, fm_lengths)\n",
    "        r_ = dict(truncated=j*10)\n",
    "        r_['run_id'] = k\n",
    "\n",
    "\n",
    "        m.fit(X_train, y_train)\n",
    "        \n",
    "        r_['accuracy_train'] = m.score(X_train, y_train)\n",
    "        r_['accuracy_test'] = m.score(X_test, y_test)\n",
    "        r_['log_loss_train'] = log_loss(y_train, m.predict_proba(X_train), labels=np.unique(y_train))\n",
    "        r_['log_loss_test'] = log_loss(y_test, m.predict_proba(X_test), labels=np.unique(y_test))\n",
    "        r_['shuffled'] = True\n",
    "        \n",
    "        runs.append(r_)\n",
    "        k += 1\n",
    "\n",
    "    classification = pd.DataFrame(runs)\n",
    "    save_path = './results/'+part+'/'+comb[0]+' vs '+comb[1]+'/'\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    classification.to_csv(save_path+'classification_morphometry_truncated_%0.2f.csv'%(j*0.1))\n",
    "    \n",
    "    # train on entire data set and save the model and data\n",
    "    X_, _ = get_pca_transformed_data(X,X,pca,fm_lengths)\n",
    "    m.fit(X_,y)\n",
    "    \n",
    "    #open(save_path+'classification_morphometry_truncated_%0.2f_model'%(j*0.1),'wb').write(pickle.dumps(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPES = ['BC', 'BPC', 'BTC', 'ChC', 'DBC', 'MC', 'NGC']\n",
    "part='full'\n",
    "truncations = np.array(range(0,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in combinations(TYPES,2):\n",
    "    with multiprocessing.Pool(10) as pool:\n",
    "        results = pool.map(partial(classify_morphometry,comb=c,part='full'),truncations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm, gaussian_kde\n",
    "def get_persistence_image(data, dim=2, t=1, bins=100, xmax=None, ymax=None):\n",
    "\n",
    "        if not data.empty:\n",
    "            if dim == 1:\n",
    "                y = np.zeros((bins,))\n",
    "                if xmax is None:\n",
    "                    xmax = np.max(data['birth'])\n",
    "                x = np.linspace(0, xmax, bins)\n",
    "\n",
    "                for k, p in data.iterrows():\n",
    "                    m = np.abs(p['birth'] - p['death'])\n",
    "                    y += m * norm.pdf(x, loc=p['birth'], scale=t)\n",
    "                return y, x\n",
    "\n",
    "            elif dim == 2:\n",
    "\n",
    "                kernel = gaussian_kde(data[['birth', 'death']].values.T)\n",
    "\n",
    "                if xmax is None:\n",
    "                    xmax = np.max(data['birth'].values)\n",
    "                if ymax is None:\n",
    "                    ymax = np.max(data['death'].values)\n",
    "                X, Y = np.mgrid[0:xmax:xmax / bins, 0:ymax:ymax / bins]\n",
    "                positions = np.vstack([X.ravel(), Y.ravel()])\n",
    "\n",
    "                Z = np.reshape(kernel(positions).T, X.shape)\n",
    "                return Z, [np.unique(X), np.unique(Y)]\n",
    "\n",
    "\n",
    "m = LogitNet(random_state=17, **{\"alpha\":0.5, \"standardize\":0})\n",
    "types = pd.DataFrame.from_csv('./data/types.csv').sort_values('c_num')\n",
    "\n",
    "\n",
    "def classify_persistence(j, comb=('BC','DBC'), part='axon'):\n",
    "    \n",
    "    path = './data/'+part+'/'\n",
    "    \n",
    "    #get data \n",
    "    data_all = pd.DataFrame.from_csv(path+'persistence_truncated_%0.2f.csv'%(j*0.1))\n",
    "    \n",
    "    idx = (types['type'] == comb[0]) | (types['type'] == comb[1])\n",
    "    data = pd.DataFrame()\n",
    "    for c in types[idx]['c_num'].values:\n",
    "        data = data.append(data_all[data_all['c_num'] == c])\n",
    "   \n",
    "    y = types[idx]['type'].values\n",
    "    y_shuffled = copy.copy(y)\n",
    "    np.random.shuffle(y_shuffled)\n",
    "    \n",
    "    c_nums = np.unique(types[idx]['c_num'])\n",
    "    max_birth = np.max(data['birth'])\n",
    "    max_death = np.max(data['death'])\n",
    "    try:\n",
    "        l = get_persistence_image(data[data['c_num'] ==c_nums[0]], xmax=max_birth, ymax=max_death)[0].reshape(-1).shape[0]\n",
    "    except np.linalg.LinAlgError as e:\n",
    "        return 'no image calculation possible anymore'\n",
    "    \n",
    "    X = np.zeros((len(c_nums),l))\n",
    "\n",
    "    for k,num in enumerate(c_nums):\n",
    "        try:\n",
    "            X[k,:] = get_persistence_image(data[data['c_num'] ==num], xmax=max_birth, ymax=max_death)[0].reshape(-1)\n",
    "        except np.linalg.LinAlgError as e:\n",
    "            return 'no image calculation possible anymore'\n",
    "    \n",
    "    X[X == np.nan] = 0\n",
    "    X[X == np.inf] = 0\n",
    "    X[X == -np.inf] = 0\n",
    "    \n",
    "    pca = PCA(copy=True, whiten=False)\n",
    "    kf = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=17)\n",
    "\n",
    "    runs = []\n",
    "    k = 1\n",
    "    fm_lengths=np.array([0,X[0].shape[0]])\n",
    "    for train_ix, test_ix in kf.split(X, y):\n",
    "\n",
    "        X_train = X[train_ix]\n",
    "        y_train = y[train_ix]\n",
    "        X_test = X[test_ix]\n",
    "        y_test = y[test_ix]\n",
    "        \n",
    "        try:\n",
    "            X_train, X_test = get_pca_transformed_data(X_train, X_test, pca, fm_lengths)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            return 'no image calculation possible anymore'\n",
    "        r_ = dict(truncated=j*10)\n",
    "        r_['run_id'] = k\n",
    "\n",
    "        m.fit(X_train, y_train)\n",
    "\n",
    "        r_['accuracy_train'] = m.score(X_train, y_train)\n",
    "        r_['accuracy_test'] = m.score(X_test, y_test)\n",
    "        r_['log_loss_train'] = log_loss(y_train, m.predict_proba(X_train), labels=np.unique(y_train))\n",
    "        r_['log_loss_test'] = log_loss(y_test, m.predict_proba(X_test), labels=np.unique(y_test))\n",
    "        r_['shuffled'] = False\n",
    "        runs.append(r_)\n",
    "        k += 1\n",
    "    k=1 \n",
    "    for train_ix, test_ix in kf.split(X, y_shuffled):\n",
    "\n",
    "        X_train = X[train_ix]\n",
    "        y_train = y_shuffled[train_ix]\n",
    "        X_test = X[test_ix]\n",
    "        y_test = y_shuffled[test_ix]\n",
    "        \n",
    "        try:\n",
    "            X_train, X_test = get_pca_transformed_data(X_train, X_test, pca, fm_lengths)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            return 'no image calculation possible anymore'\n",
    "        r_ = dict(truncated=j*10)\n",
    "        r_['run_id'] = k\n",
    "\n",
    "        m.fit(X_train, y_train)\n",
    "\n",
    "        r_['accuracy_train'] = m.score(X_train, y_train)\n",
    "        r_['accuracy_test'] = m.score(X_test, y_test)\n",
    "        r_['log_loss_train'] = log_loss(y_train, m.predict_proba(X_train), labels=np.unique(y_train))\n",
    "        r_['log_loss_test'] = log_loss(y_test, m.predict_proba(X_test), labels=np.unique(y_test))\n",
    "        r_['shuffled'] = True\n",
    "        runs.append(r_)\n",
    "        k += 1\n",
    "\n",
    "    classification = pd.DataFrame(runs)\n",
    "    save_path = './results/'+part+'/'+comb[0]+' vs '+comb[1]+'/'\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "    classification.to_csv(save_path+'classification_persistence_truncated_%0.2f.csv'%(j*0.1))\n",
    "    \n",
    "    # train on entire data set and save the model and data\n",
    "    try:\n",
    "        X_, _ = get_pca_transformed_data(X,X,pca,fm_lengths)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return 'no image calculation possible for all data'\n",
    "    m.fit(X_,y)\n",
    "    \n",
    "    #open(save_path + 'classification_persistence_truncated_%0.2f_model'%(j*0.1),'wb').write(pickle.dumps(m))\n",
    "    #open(save_path + 'classification_persistence_truncated_%0.2f_pca'%(j*0.1),'wb').write(pickle.dumps(pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in list(combinations(TYPES,2)):\n",
    "    with multiprocessing.Pool(5) as pool:\n",
    "        try:\n",
    "            results = pool.map(partial(classify_persistence,comb=c,part='full'),truncations)\n",
    "        except ValueError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Density maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _project_data(dim, proj_axes, data):\n",
    "\n",
    "    p_a = proj_axes\n",
    "    if dim == 2:\n",
    "        if len(p_a) < 2:\n",
    "            print('Invalid parameter setting: The passed projection axes {0} do not \\\n",
    "                  fit with the dimension of the projection {1}'.format(p_a, dim))\n",
    "        else:\n",
    "            indices = '012'\n",
    "            for ix in range(len(p_a)):\n",
    "                indices = indices.replace(p_a[ix], '')\n",
    "            deleted_axis = int(indices)\n",
    "            ax = [0, 1, 2]\n",
    "            ax.remove(deleted_axis)\n",
    "            result = data[:, ax]\n",
    "\n",
    "    elif dim == 1:\n",
    "        if len(p_a) > 1:\n",
    "            print('Invalid parameter setting: The passed projection axes {0} do not \\\n",
    "                  fit with the dimension of the projection {1}'.format(p_a, dim))\n",
    "            \n",
    "        else:\n",
    "            ax = int(p_a)\n",
    "            result = data[:, ax]\n",
    "    else:\n",
    "        result = data\n",
    "\n",
    "    return result\n",
    "\n",
    "from utils.utils import smooth_gaussian\n",
    "def get_density_map(pc,proj_axes, r):\n",
    "    \n",
    "\n",
    "    dim = len(proj_axes)\n",
    "    pc = (pc - r['min'][0]) / (r['max'][0] - r['min'][0])\n",
    "\n",
    "    data = _project_data(dim, proj_axes, pc)\n",
    "\n",
    "    range_ = [[-.1, 1.1]] * dim\n",
    "    H, edges = np.histogramdd(data, bins=(100,) * dim,\n",
    "                              range=range_, normed=True)\n",
    "\n",
    "    H = smooth_gaussian(H, dim=dim, sigma=2)\n",
    "    return H\n",
    "\n",
    "\n",
    "m = LogitNet(random_state=17, **{\"alpha\":0.5, \"standardize\":0})\n",
    "\n",
    "def classify_density_map(j, comb=('BC','DBC'), part='axon'):\n",
    "    \n",
    "    path = './data/'+part+'/'\n",
    "    \n",
    "    #get data \n",
    "    data_all = pd.DataFrame.from_csv(path+'point_cloud_truncated_%0.2f.csv'%(j*0.1))\n",
    "    \n",
    "    idx = (types['type'] == comb[0]) | (types['type'] == comb[1])\n",
    "    data = pd.DataFrame()\n",
    "    for c in types[idx]['c_num'].values:\n",
    "        data = data.append(data_all[data_all['c_num'] == c])\n",
    "   \n",
    "    y = types[idx]['type'].values\n",
    "    y_shuffled = copy.copy(y)\n",
    "    np.random.shuffle(y_shuffled)\n",
    "    \n",
    "    c_nums = np.unique(types[idx]['c_num'])\n",
    "    r = dict(min=np.min(data_all[['x','y','z']]).values, max=np.max(data_all[['x','y','z']]).values)\n",
    "    X = np.zeros((len(c_nums),10000))\n",
    "\n",
    "    for k,num in enumerate(c_nums):\n",
    "        \n",
    "        D = data[data['c_num'] ==num][['x','y','z']].values\n",
    "        X[k,:] = get_density_map(D, '02', r).reshape(-1)\n",
    "\n",
    "    X[X == np.nan] = 0\n",
    "    X[X == np.inf] = 0\n",
    "    X[X == -np.inf] = 0\n",
    "    \n",
    "    \n",
    "    #### CLASSIFICATION ######\n",
    "    pca = PCA(copy=True, whiten=False)    \n",
    "    kf = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=17)\n",
    "\n",
    "    runs = []\n",
    "    k = 1\n",
    "    fm_lengths=np.array([0,X[0].shape[0]])\n",
    "    for train_ix, test_ix in kf.split(X, y):\n",
    "\n",
    "        X_train = X[train_ix]\n",
    "        y_train = y[train_ix]\n",
    "        X_test = X[test_ix]\n",
    "        y_test = y[test_ix]\n",
    "        \n",
    "        try:\n",
    "            X_train, X_test = get_pca_transformed_data(X_train, X_test, pca, fm_lengths)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            return 'no image calculation possible anymore'\n",
    "        r_ = dict(truncated=j*10)\n",
    "        r_['run_id'] = k\n",
    "\n",
    "        m.fit(X_train, y_train)\n",
    "\n",
    "        r_['accuracy_train'] = m.score(X_train, y_train)\n",
    "        r_['accuracy_test'] = m.score(X_test, y_test)\n",
    "        r_['log_loss_train'] = log_loss(y_train, m.predict_proba(X_train), labels=np.unique(y_train))\n",
    "        r_['log_loss_test'] = log_loss(y_test, m.predict_proba(X_test), labels=np.unique(y_test))\n",
    "        r_['shuffled'] = False\n",
    "        runs.append(r_)\n",
    "        k += 1\n",
    "    k=1 \n",
    "    for train_ix, test_ix in kf.split(X, y_shuffled):\n",
    "\n",
    "        X_train = X[train_ix]\n",
    "        y_train = y_shuffled[train_ix]\n",
    "        X_test = X[test_ix]\n",
    "        y_test = y_shuffled[test_ix]\n",
    "        \n",
    "        try:\n",
    "            X_train, X_test = get_pca_transformed_data(X_train, X_test, pca, fm_lengths)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            return 'no image calculation possible anymore'\n",
    "        r_ = dict(truncated=j*10)\n",
    "        r_['run_id'] = k\n",
    "\n",
    "        m.fit(X_train, y_train)\n",
    "\n",
    "        r_['accuracy_train'] = m.score(X_train, y_train)\n",
    "        r_['accuracy_test'] = m.score(X_test, y_test)\n",
    "        r_['log_loss_train'] = log_loss(y_train, m.predict_proba(X_train), labels=np.unique(y_train))\n",
    "        r_['log_loss_test'] = log_loss(y_test, m.predict_proba(X_test), labels=np.unique(y_test))\n",
    "        r_['shuffled'] = True\n",
    "        runs.append(r_)\n",
    "        k += 1\n",
    "\n",
    "    classification = pd.DataFrame(runs)\n",
    "    save_path = './results/'+part+'/'+comb[0]+' vs '+comb[1]+'/'\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "    classification.to_csv(save_path+'classification_density_map_truncated_%0.2f.csv'%(j*0.1))\n",
    "    \n",
    "    # train on entire data set and save the model and data\n",
    "    try:\n",
    "        X_, _ = get_pca_transformed_data(X,X,pca,fm_lengths)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return 'no image calculation possible for all data'\n",
    "    m.fit(X_,y)\n",
    "    \n",
    "    #open(save_path + 'classification_density_map_truncated_%0.2f_model'%(j*0.1),'wb').write(pickle.dumps(m))\n",
    "    #open(save_path + 'classification_density_map_truncated_%0.2f_pca'%(j*0.1),'wb').write(pickle.dumps(pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in list(combinations(TYPES,2)):\n",
    "    with multiprocessing.Pool(5) as pool:\n",
    "        try:\n",
    "            results = pool.map(partial(classify_density_map,comb=c,part='full'),truncations)\n",
    "        except ValueError:\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
